{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":110281,"databundleVersionId":13391012,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<h1 style=\"color:rgb(233, 92, 32);\"><center><font>From Noise to Decision: Detecting Statistically Significant Market Moves with BigQuery AI</font></center></h1>\n\n<h2 style=\"color:rgb(96, 63, 131);border-radius:5px;display:fill\"><center><font>“Risk comes from not knowing what you’re doing”</font></center></h2>\n\n\n1. [Goal](#001)\n2. [Problem Statement](#002)\n3. [Impact Statement](#003)\n4. [Architecture](#004)\n5. [Custom Algorithm](#005)\n6. [BigQuery AI Usage](#006)\n7. [Current, Future & Limitations](#007)\n8. [Why DSSMM (Detecting Statistically Significant Market Moves)](#008)\n9. [Target Audience](#009)\n10. [Project Setup & Run Guide](#010)\n11. [Interactive Dashboard](#011)\n12. [Survey — BigQuery AI: Building the Future of Data](#012)\n\n\n\n# <div style=\"color:rgb(0, 103, 71);display:fill;border-radius:5px;background-color:whie;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:10px;color:rgb(0, 103, 71);overflow:hidden;margin:0;font-size:100%; \"><b id= '001'>Goal </b> </p></div>\n\n\nTo **cut through market noise and determine whether the market move is statistically significant and why**, using statistical data-backed reasoning. The system delivers a clear verdict (Signal / Weak / Noise) supported by statistical analysis, backtesting, market news, and stock value movements. Each output includes a concise 2-4 line blurb explanation, powered by BigQuery and leveraging a combined approach of **AI Architect** and **Semantic Detective**.\n\n\n# <div style=\"color:rgb(0, 103, 71);display:fill;border-radius:5px;background-color:whie;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:10px;color:rgb(0, 103, 71);overflow:hidden;margin:0;font-size:100%; \"><b id= '002'>Problem Statement </b> </p></div>\n\nMost day-to-day stock market fluctuations are nothing more than noise. Analysts and quants are buried under endless headlines, dashboards, and conflicting signals. Existing tools either flag everything as \"important\" or require hours of manual investigation, slowing decision-making when speed matters most.\n\nThis creates an urgent need for a direct, decision-ready answer in plain English:\n\n> **\"Did this market move actually matter, and what caused it?\"**\n\nOur proposal delivers a concise **Signal vs. Noise** verdict, supported by statistical rigor, backtesting, and contextual market intelligence. A prototype demonstrates how raw price data, unstructured news and other relevant content can be transformed into clear, investor-grade insights, **directly inside BigQuery**, powered by its **Generative AI** and **vector search** capabilities.\n\n\n# <div style=\"color:rgb(0, 103, 71);display:fill;border-radius:5px;background-color:whie;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:10px;color:rgb(0, 103, 71);overflow:hidden;margin:0;font-size:100%; \"><b id= '003'>Impact Statement </b> </p></div>\n\n\nOur goal cuts triage time, improves consistency, and scales cleanly inside BigQuery with full auditability for risk and governance. Built on BigQuery-native **custom composite S³ (Signal Strength Score)** which blends shock size (z-scores), news corroboration, and AI tone into one transparent metric.\n\n- **What sets our solution apart?**\n\n  > Most tools only show stock moves (up/down), and a few flag “signals,” but rarely explain _why_. Our approach combines **statistical testing** (was the move real or just noise?) with **relevant news & headline context** (earnings, guidance, macro events, or flows). The result is both the **proof** (the move is real) and the **reason** (what likely drove it)—delivered in plain English.\n\n- **Faster decisions:**\n\n  > Analysts receive a **signal vs. noise verdict** plus a **2-4 sentence explanation**, without hours of manual digging.\n\n- **Consistent triage:**\n\n  > Objective **z-score tests** and a transparent **scoring model** cut bias and reduce subjective guesswork.\n\n- **Ready for scale:**\n\n  > The demo runs on **public data feeds** (yfinance + RSS) but is designed to expand to **official sources** (Nasdaq, government filings, premium news, etc.).\n\n- **Time saved:**\n  > Reclaims hundreds of analyst and quant hours each year by eliminating ad-hoc investigations. Also includes a **light backtest view (5- and 10-day forward returns)** to validate whether **“signal”** days behave differently from **“noise.”**\n\n\n# <div style=\"color:rgb(0, 103, 71);display:fill;border-radius:5px;background-color:whie;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:10px;color:rgb(0, 103, 71);overflow:hidden;margin:0;font-size:100%; \"><b id= '004'>Prototype Architecture </b> </p></div>\n\n\n<center><img src=\"https://i.postimg.cc/rmVPYGdd/dssmm.png\"></center>\n\n\nA **BigQuery-native pipeline** that:\n\n1. **Ingests daily prices** of tickers.\n2. **Flags only meaningful shocks** using **z-scores on returns** and **abnormal returns vs. market benchmarks**.\n3. **Connects events to context** by matching them with **semantically similar news** via **vector embeddings**.\n4. **Generates investor-style insights**, a short explanation and sentiment tilt, through **BigQuery Generative AI**.\n\n\n# <div style=\"color:rgb(0, 103, 71);display:fill;border-radius:5px;background-color:whie;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:10px;color:rgb(0, 103, 71);overflow:hidden;margin:0;font-size:100%; \"><b id= '005'>Custom Algorithm </b> </p></div>\n\n\n1. **Price move → Is it meaningful?**\n   - Compute **daily return** and a **market-adjusted abnormal return** (via simple regression against the index in **BigQuery ML**).\n   - Assign **z-scores** (how many standard deviations away from normal).\n   - If **|z| ≥ ~1.96** (≈ **95% confidence**), flag the date as a **Signal Candidate**.\n\n\n2. **What explains it?**\n   - Embed recent headlines and a short **“ticker context”** text using **`ML.GENERATE_EMBEDDING`**.\n   - Compute **cosine similarity** directly in SQL to surface the best matches around the event window (**±2 days**).\n   - If no strong match is found, fall back to **custom keywords** (fully parameterized per ticker).\n\n\n3. **Explain like an analyst**\n   - Feed the matched headlines to **`ML.GENERATE_TEXT`** with a concise, plain-English prompt:\n     - “**Explain in simple investor language… use terms like beat/miss… keep it to 2-4 sentences.**”\n   - Separately, classify sentiment with a **True/False** check: _“Net positive for equity holders?”_ → stored as **`is_positive`**.\n\n\n4. **Final verdict & quick validation**\n   - Assign a **Signal Score** (combining shock size, headline strength, and AI tone) → label as **Signal / Weak / Noise**.\n   - Run a light **5-day forward return check** to see if **Signal days** behave differently from **Noise days**.\n\n\n<h2 style=\"color:rgb(233, 92, 32);border-radius:5px;display:fill\"><center><font>DSSMM - Triage S³ (Signal Strength Score) Formula</font></center></h2>\n\n\n**Inputs (prototype):**\n\n- **Prices:** `yfinance` (daily OHLC)\n- **News:** public RSS (Reuters / WSJ Markets / Yahoo Finance)\n- **Custom Keywords:** Your custom keywords in regards to the ticker we select\n\n**Processing (in BigQuery):**\n\n- **Stat tests:** daily return, market-adjusted **abnormal return** (BQML linear reg vs. index), and **z-scores**:\n  - `z_day` = z-score of the raw daily return\n  - `z_ar` = z-score of the abnormal (market-adjusted) return\n- **Semantic matching:** `ML.GENERATE_EMBEDDING` on headlines + ticker context → cosine similarity to link events ↔ news\n- **Summarization & tone:** `ML.GENERATE_TEXT` → 2–4 sentence explanation; plus a **True/False** tilt for equity holders\n- **Count of supporting news:** `news_count` near the event window (±2 days)\n\n---\n\n$$\n\\mathbf{signal\\_score}\n= 0.4 \\cdot \\frac{\\min\\!\\big(\\max(|z_{day}|,|z_{ar}|),\\,4\\big)}{4}\n+ 0.4 \\cdot \\frac{\\min(\\text{news\\_count},\\,5)}{5}\n+ 0.2 \\cdot \\frac{(\\text{ai\\_tilt}+1)}{2}\n$$\n\n---\n  \n**Where:**\n\n- \\(|z_day|\\), \\(|z_ar|\\) capture shock size (capped at 4σ for stability)\n- `news_count` is capped at 5 (diminishing returns)\n- `ai_tilt` ∈ \\{-1, 0, +1\\} (negative / neutral / positive)\n\n**Labels (for triage):**\n\n- **Signal** if `signal_score ≥ 0.65`\n- **Weak** if `0.40 ≤ signal_score < 0.65`\n- **Noise** otherwise\n\n**Outputs:**\n\n- **Verdict:** _Signal / Weak / Noise_\n- **Why it moved:** 2–4 sentence summary with links to top-matching news & headlines\n- **Sanity check:** 5-day forward return table (diagnostic)\n\n\n# <div style=\"color:rgb(0, 103, 71);display:fill;border-radius:5px;background-color:whie;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:10px;color:rgb(0, 103, 71);overflow:hidden;margin:0;font-size:100%; \"><b id= '006'>Approach & How BigQuery AI Was Used </b> </p></div>\n\n\nWe combined two competition approaches: **The AI Architect** & **The Semantic Detective**\n\n1. **The AI Architect (Generative AI in SQL)**\n   - **`ML.GENERATE_TEXT`:**\n     - a brief **investor explanation** of the day’s move (**2–4 sentences**),\n     - a **binary tone** (**net positive / net negative** for equity holders) via a constrained prompt.\n   - **Why inside BigQuery?** No ETL round-trips. Prompts run where the data lives; we **store outputs in tables** for auditing.\n\n\n2. **The Semantic Detective (Embeddings & Similarity)**\n   - **`ML.GENERATE_EMBEDDING`:**\n     - aggregated **news content** (title + summary), and\n     - compact **ticker context blobs** (e.g., median z-scores) so similarity is about the ticker’s **recent regime**, not just keywords.\n   - We compute **cosine similarity** inside **BigQuery** to link **events ↔ headlines**.\n   - A **keyword fallback** (simple, transparent) ensures recall even when embeddings are sparse.\n\n> **Multimodal note (prototype):** our objects today are **text**; the same pattern works with **screenshots/PDFs** via **Object Tables** in production.\n\n\n# <div style=\"color:rgb(0, 103, 71);display:fill;border-radius:5px;background-color:whie;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:10px;color:rgb(0, 103, 71);overflow:hidden;margin:0;font-size:100%; \"><b id= '007'>Current, Future & Limitations </b> </p></div>\n\n- **Prototype inputs (public):**\n  - **Prices:** **yfinance** (AAPL by default; parameterized for any ticker list).\n  - **News:** public **RSS feeds** (Reuters, WSJ Markets, Yahoo Finance).\n  - **Custom Keywords:** Your desired keywords in regards to the ticker.\n\n\n- **Production-ready substitutes:**\n  - **Official price/reference data** (Nasdaq, exchanges).\n  - **Trusted news & filings** (.gov, EDGAR, premium feeds).\n  - **Screenshots/PDFs** via **Object Tables** and **ObjectRef** for **multimodal** expansion.\n\n\n- **Limitations (today):**\n    - Running on **public feeds** at an end-of-day cadence means this prototype isn’t suited for regulated or latency-sensitive decisions.\n    - **Free-credit constraints** limits scale\n    - Sparse headlines can under-explain some moves until official sources are enabled.\n    - Governance (prompt/model versioning) is planned for production hardening.\n\n\n# <div style=\"color:rgb(0, 103, 71);display:fill;border-radius:5px;background-color:whie;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:10px;color:rgb(0, 103, 71);overflow:hidden;margin:0;font-size:100%; \"><b id= '008'>Why DSSMM </b> </p></div>\n\n\n- **Warehouse-native unstructured intelligence:** We stay in **BigQuery**—no separate vector DB, no bespoke microservices. Teams can **audit prompts and results** like any other table.\n- **“Explain why” by default:** Not just “price moved,” but a **compact, human summary** tied to **concrete headlines**.\n- **Semantics > keywords:** **Embeddings** make “AI + SQL” useful on messy text without building an external search stack.\n- **Parameterizable & multi-tenant:** One pipeline, many users—**tickers and keywords** passed per user/team.\n\n\n# <div style=\"color:rgb(0, 103, 71);display:fill;border-radius:5px;background-color:whie;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:10px;color:rgb(0, 103, 71);overflow:hidden;margin:0;font-size:100%; \"><b id= '009'>Target Audience </b> </p></div>\n\n\n- **Equity analysts & portfolio managers:** fast daily **triage** of stock moves across a watchlist.\n- **IR/Comms teams & executives:** a **plain-English “reason why”** summary for major price moves.\n- **Ops & data teams:** a **ready-made template** for bringing unstructured data into a **warehouse-native AI workflow**.\n\n\n# <div style=\"color:rgb(0, 103, 71);display:fill;border-radius:5px;background-color:whie;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:10px;color:rgb(0, 103, 71);overflow:hidden;margin:0;font-size:100%; \"><b id= '010'>Project Setup & Run Guide </b> </p></div>\n\n\n**★ Google Cloud project**:\n- **Pick or create a project** in Google Cloud Console. Copy the **Project ID** (e.g., `my-project-12345`).\n- **Turn on APIs**: In that project, enable **Vertex AI API** and **BigQuery Connection API**.\n- **Make a service account** (IAM & Admin → Service Accounts → **Create**): name it like `kaggle-runner`.\n  - Give these roles (*least privilege*): **BigQuery Admin, Vertex AI User, Service Usage Admin**.\n    - Open the new account → **Keys** → **Add key** → **Create new key** → JSON. A key file downloads.\n\n**★ Kaggle Notebook**:\n- In Kaggle, open your notebook → **Add-ons** → **Secrets**.\n- Add two secrets:\n  - `GCP_PROJECT_ID` = your Project ID.\n  - `GCP_SA_KEY` = open the downloaded JSON file and paste the whole content.\n- Configure Google Cloud SDK → **Add-ons** → **Google Cloud SDK**\n\n**★ First Run**: grant connection access\n- Run the notebook setup cells. This creates a BigQuery external connection (named `llm-connection`).\n- In Google Cloud **Console** → **BigQuery** → **External connections**, open llm-connection and copy its Service Account (looks like `bqcx-...@...gserviceaccount.com`).\n- Go to **IAM & Admin** → **Grant Access**: paste that service account, give it **Vertex AI User**, and **Save**.\n\nThat’s it—your notebook can now securely use BigQuery + Vertex AI for the analysis. Setup notes were inspired by the [Kaggle notebook](https://www.kaggle.com/code/fissalalsharef/bigquery-ai-the-patent-analyst-project#Google-Cloud-Project-Setup). Thank you to the author and Kaggle community\n\n\n- Setup Creds and Configure\n\n\n```python\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nproject_id = user_secrets.get_secret(\"GCP_PROJECT_ID\")\ngcp_key_json = user_secrets.get_secret(\"GCP_SA_KEYS\")\nlocation = 'us'\n\nimport os\nkey_file_path = 'gcp_key.json'\ntry:\n    with open(key_file_path, 'w') as f:\n        f.write(gcp_key_json)\n    \n    !gcloud auth activate-service-account --key-file={key_file_path} > /dev/null 2>&1\n    !gcloud config set project {project_id} > /dev/null 2>&1\n    \nfinally:\n    if os.path.exists(key_file_path):\n        os.remove(key_file_path)\n\n```\n\n\n- After executing the above code, make a connection to GCP by running the following commands\n\n\n```sh\n!bq mk --connection --location={location} --connection_type=CLOUD_RESOURCE llm-connection > /dev/null 2>&1\n!bq show --connection --location={location} llm-connection > /dev/null 2>&1\n```\n\nFree-credit limits mean the live demo is constrained. We recommend using **your own GCP credentials & configs** and running the notebook for your tickers. \n\nThis code sets up BigQuery remote GenAI models, ingests prices & news, computes z-scores/abnormal returns, links events↔headlines via embeddings/keywords, generates 2–4 sentence summaries + tone, labels days as Signal/Weak/Noise, and produces a 5-day backtest.\n\n\n```python\nfrom __future__ import annotations\n\nimport os\nimport re\nimport uuid\nimport datetime as dt\nfrom dataclasses import dataclass\nfrom typing import Optional, List, Dict, Any, Iterable\n\nimport pandas as pd\nimport pandas_gbq\nfrom pandas_gbq import to_gbq\n\nimport yfinance as yf\nfrom dateutil import parser as dateparser\nfrom google.cloud import bigquery\n\ntry:\n    from IPython.display import display \n    _HAS_DISPLAY = True\nexcept Exception:\n    _HAS_DISPLAY = False\n\n\n@dataclass\nclass BQConfig:\n    project_id: str = os.getenv(\"GCP_PROJECT_ID\")\n    location:   str = os.getenv(\"GCP_LOCATION\")\n    dataset_id: str = os.getenv(\"BQ_DATASET\")\n    connection_id: str = os.getenv(\"CONNECTION_ID\")\n\n\nclass MarketSignalPipeline:\n    \"\"\"\n    Universal, parameterized wrapper around your end-to-end pipeline.\n    \"\"\"\n\n    def __init__(\n        self,\n        cfg: Optional[BQConfig] = None,\n        news_lookback_days: int = 120,\n        cosine_min: float = 0.60,\n        window_days: int = 2,\n        news_feeds: Optional[Dict[str, str]] = None\n    ):\n        self.cfg = cfg or BQConfig()\n        self.PROJECT_ID = self.cfg.project_id\n        self.LOCATION   = self.cfg.location\n        self.DATASET_ID = self.cfg.dataset_id\n        self.BQDFT      = f\"`{self.PROJECT_ID}.{self.DATASET_ID}`\"\n\n        self.client = bigquery.Client(project=self.PROJECT_ID, location=self.LOCATION)\n        self.client.create_dataset(\n            bigquery.Dataset(f\"{self.PROJECT_ID}.{self.DATASET_ID}\"),\n            exists_ok=True\n        )\n        print(\"✅ BigQuery ready:\", self.PROJECT_ID, self.DATASET_ID, \"in\", self.LOCATION)\n\n        # Connection candidates\n        region_lower = self.LOCATION.lower()\n        self.CANDIDATES = [f\"{region_lower}.llm-connection\"]\n\n        # Connection id from external connection of BigQuery\n        self.CONNECTION_ID = self.cfg.connection_id\n\n        # GEN AI Models \n        self.EMB_MODEL: Optional[str] = None\n        self.GEN_MODEL: Optional[str] = None\n\n        # Tunables\n        self.NEWS_LOOKBACK_DAYS = news_lookback_days\n        self.COSINE_MIN         = cosine_min\n        self.WINDOW_DAYS        = window_days\n\n        # News feeds we can feed this with premium as well\n        self.NEWS_FEEDS: Dict[str, str] = news_feeds or {\n            \"Reuters Business\": \"https://feeds.reuters.com/reuters/businessNews\",\n            \"WSJ Markets\": \"https://feeds.a.dj.com/rss/RSSMarketsMain.xml\",\n            \"Yahoo Finance\": \"https://finance.yahoo.com/rss/topstories\",\n        }\n\n        # Keywords (set per-run via set_keywords / run_all)\n        self.TICKER_KEYWORDS: Dict[str, List[str]] = {}\n\n    # ------------------------- #\n    #      Setup / Models       #\n    # ------------------------- #\n    def try_create_models(self, conn_id: str) -> bool:\n        try:\n            ddl = f\"\"\"\n            CREATE SCHEMA IF NOT EXISTS `{self.PROJECT_ID}.{self.DATASET_ID}` OPTIONS(location='{self.LOCATION}');\n    \n            CREATE OR REPLACE MODEL `{self.PROJECT_ID}.{self.DATASET_ID}.ms_gen`\n              REMOTE WITH CONNECTION `{conn_id}`\n              OPTIONS (endpoint = 'gemini-2.5-flash');\n    \n            CREATE OR REPLACE MODEL `{self.PROJECT_ID}.{self.DATASET_ID}.ms_embed`\n              REMOTE WITH CONNECTION `{conn_id}`\n              OPTIONS (endpoint = 'text-embedding-004');\n            \"\"\"\n            self.client.query(ddl).result()\n            print(\"✅ Remote models created via connection:\", conn_id)\n            return True\n        except Exception as e:\n            print(f\"⚠️ Could not create models with {conn_id}: {e}\")\n            return False\n\n    def setup_models(self) -> None:\n        for c in self.CANDIDATES:\n            if self.try_create_models(c):\n                self.CONNECTION_ID = c\n                break\n\n        if self.CONNECTION_ID is None:\n            raise RuntimeError(\"No working external connection found. Check your connection name/region & permissions.\")\n\n        self.EMB_MODEL = f\"{self.PROJECT_ID}.{self.DATASET_ID}.ms_embed\"\n        self.GEN_MODEL = f\"{self.PROJECT_ID}.{self.DATASET_ID}.ms_gen\"\n        print(\"EMB_MODEL =\", self.EMB_MODEL)\n        print(\"GEN_MODEL =\", self.GEN_MODEL)\n\n    # ------------------------- #\n    #  Price loading & returns  #\n    # ------------------------- #\n    def _prices_table_exists(self) -> bool:\n        try:\n            self.client.get_table(f\"{self.PROJECT_ID}.{self.DATASET_ID}.prices\")\n            return True\n        except Exception:\n            return False\n\n    def load_prices_to_bq(\n        self,\n        ticker: str,\n        benchmark: str = \"^GSPC\",\n        start: str = \"2015-01-01\",\n        end: Optional[str] = None,\n        if_exists_strategy: Optional[str] = None,\n    ) -> str:\n        \"\"\"\n        Loads prices for a single ticker and benchmark.\n        - Preserves your original transform.\n        - Will 'replace' if table doesn't exist, otherwise 'append' (unless overridden).\n        \"\"\"\n        end = end or dt.date.today().isoformat()\n        df = yf.download([ticker, benchmark], start=start, end=end, auto_adjust=True, progress=False)\n        if df.empty:\n            raise ValueError(f\"No price data returned for {ticker}. Check symbols or dates.\")\n        df = df[\"Close\"].reset_index().rename(columns={ticker: \"close\", benchmark: \"bench_close\"})\n        df[\"ticker\"] = ticker\n        df[\"benchmark\"] = benchmark\n        df.rename(columns={\"Date\": \"date\"}, inplace=True)\n\n        # Decide write mode (append for additional tickers)\n        if_exists = if_exists_strategy or (\"append\" if self._prices_table_exists() else \"replace\")\n        pandas_gbq.to_gbq(df, f\"{self.DATASET_ID}.prices\", project_id=self.PROJECT_ID, if_exists=if_exists)\n        table = f\"{self.PROJECT_ID}.{self.DATASET_ID}.prices\"\n        print(f\"✅ Loaded prices to: {table}  (mode={if_exists})\")\n        return table\n\n    def load_prices_many(\n        self,\n        tickers: Iterable[str],\n        benchmark: str = \"^GSPC\",\n        start: str = \"2018-01-01\",\n        end: Optional[str] = None,\n    ) -> None:\n        tickers = list(tickers)\n        if not tickers:\n            raise ValueError(\"No tickers provided.\")\n        for i, t in enumerate(tickers):\n            # first ticker can 'replace', others will append automatically\n            self.load_prices_to_bq(\n                ticker=t,\n                benchmark=benchmark,\n                start=start,\n                end=end,\n                if_exists_strategy=(\"replace\" if i == 0 and not self._prices_table_exists() else \"append\"),\n            )\n\n    def build_daily_returns(self) -> None:\n        sql_returns = f\"\"\"\n        CREATE OR REPLACE TABLE {self.BQDFT}.daily_returns AS\n        WITH base AS (\n          SELECT\n            ticker,\n            DATE(date) AS date,\n            close,\n            bench_close,\n            SAFE_DIVIDE(close - LAG(close) OVER (PARTITION BY ticker ORDER BY date),\n                        LAG(close) OVER (PARTITION BY ticker ORDER BY date)) AS r,\n            SAFE_DIVIDE(bench_close - LAG(bench_close) OVER (PARTITION BY ticker ORDER BY date),\n                        LAG(bench_close) OVER (PARTITION BY ticker ORDER BY date)) AS r_mkt\n          FROM {self.BQDFT}.prices\n        )\n        SELECT\n          *,\n          STDDEV_SAMP(r)     OVER (PARTITION BY ticker ORDER BY date ROWS BETWEEN 20 PRECEDING AND 1 PRECEDING) AS vol20,\n          STDDEV_SAMP(r_mkt) OVER (PARTITION BY ticker ORDER BY date ROWS BETWEEN 120 PRECEDING AND 1 PRECEDING) AS vol_mkt120,\n          SAFE_DIVIDE(r, NULLIF(STDDEV_SAMP(r) OVER (PARTITION BY ticker ORDER BY date ROWS BETWEEN 20 PRECEDING AND 1 PRECEDING),0)) AS z_day\n        FROM base;\n        \"\"\"\n        self.client.query(sql_returns).result()\n        print(\"✅ daily_returns created\")\n\n    def build_linear_model(self) -> None:\n        create_model_sql = f\"\"\"\n        CREATE OR REPLACE MODEL `lunar-geography-472503-n7.noise_to_decision_with_ai.mm_linear_regression`\n        OPTIONS(model_type='linear_reg', input_label_cols=['r']) AS\n        SELECT r, r_mkt\n        FROM `lunar-geography-472503-n7.noise_to_decision_with_ai.daily_returns`\n        WHERE r IS NOT NULL AND r_mkt IS NOT NULL;\n        \"\"\"\n        self.client.query(create_model_sql).result()\n        print(\"✅ Model created\")\n\n    def build_abnormal_returns(self) -> None:\n        sql_ar = f\"\"\"\n        CREATE OR REPLACE TABLE {self.BQDFT}.abnormal_returns AS\n        WITH pred AS (\n          SELECT\n            ticker, date, r, r_mkt,\n            predicted_r AS r_hat\n          FROM ML.PREDICT(\n            MODEL {self.BQDFT}.mm_linear_regression,\n            (SELECT ticker, date, r, r_mkt\n             FROM {self.BQDFT}.daily_returns\n             WHERE r IS NOT NULL AND r_mkt IS NOT NULL)\n          )\n        ),\n        stats AS (\n          SELECT\n            ticker,\n            date,\n            r,\n            r_mkt,\n            r_hat,\n            (r - r_hat) AS ar,\n            -- trailing 120 trading days (excluding today)\n            AVG(r) OVER (\n              PARTITION BY ticker\n              ORDER BY date\n              ROWS BETWEEN 120 PRECEDING AND 1 PRECEDING\n            ) AS mean_r_120,\n            STDDEV_SAMP(r) OVER (\n              PARTITION BY ticker\n              ORDER BY date\n              ROWS BETWEEN 120 PRECEDING AND 1 PRECEDING\n            ) AS sd_r_120,\n            STDDEV_SAMP(r - r_hat) OVER (\n              PARTITION BY ticker\n              ORDER BY date\n              ROWS BETWEEN 120 PRECEDING AND 1 PRECEDING\n            ) AS sd_ar_120\n          FROM pred\n        )\n        SELECT\n          ticker,\n          date,\n          r,\n          r_mkt,\n          r_hat,\n          ar,\n          SAFE_DIVIDE(ar, sd_ar_120)                    AS z_ar,\n          SAFE_DIVIDE(r - mean_r_120, sd_r_120)         AS z_day\n        FROM stats;\n        \"\"\"\n        self.client.query(sql_ar).result()\n        print(\"✅ abnormal_returns (with z_day) created\")\n\n    def build_trend_10d(self) -> None:\n        sql_trend = f\"\"\"\n        CREATE OR REPLACE TABLE {self.BQDFT}.trend_10d AS\n        WITH base AS (\n          SELECT\n            ticker,\n            date,\n            close,\n            SAFE.LOG(close) AS ln_p,\n            ROW_NUMBER() OVER (PARTITION BY ticker ORDER BY date) AS t\n          FROM {self.BQDFT}.prices\n          WHERE close IS NOT NULL AND close > 0\n        ),\n        roll AS (\n          SELECT\n            ticker,\n            date,\n            COUNT(*)        OVER w AS n,\n            SUM(t)          OVER w AS sum_t,\n            SUM(ln_p)       OVER w AS sum_y,\n            SUM(t*t)        OVER w AS sum_tt,\n            SUM(ln_p*t)     OVER w AS sum_ty,\n            SUM(ln_p*ln_p)  OVER w AS sum_yy\n          FROM base\n          WINDOW w AS (\n            PARTITION BY ticker\n            ORDER BY date\n            ROWS BETWEEN 9 PRECEDING AND CURRENT ROW\n          )\n        ),\n        fit AS (\n          SELECT\n            ticker,\n            date,\n            n,\n            (sum_tt - (sum_t*sum_t)/CAST(n AS FLOAT64)) AS sxx,\n            (sum_ty - (sum_t*sum_y)/CAST(n AS FLOAT64)) AS sxy,\n            (sum_yy - (sum_y*sum_y)/CAST(n AS FLOAT64)) AS syy\n          FROM roll\n        ),\n        stats AS (\n          SELECT\n            ticker,\n            date,\n            n,\n            sxx,\n            sxy,\n            syy,\n            SAFE_DIVIDE(sxy, sxx) AS slope,\n            CASE\n              WHEN n >= 3 AND sxx > 0 THEN\n                SAFE_DIVIDE(syy - SAFE_DIVIDE(sxy*sxy, sxx), n - 2)\n            END AS s2\n          FROM fit\n        )\n        SELECT\n          ticker,\n          date,\n          n,\n          slope,\n          s2,\n          CASE\n            WHEN n >= 3 AND sxx > 0 AND s2 IS NOT NULL\n              THEN SAFE_DIVIDE(slope, SQRT(SAFE_DIVIDE(s2, sxx)))\n            ELSE NULL\n          END AS t_stat\n        FROM stats;\n        \"\"\"\n        self.client.query(sql_trend).result()\n        print(\"✅ trend_10d created\")\n\n    # -------------------------\n    #       News ingest\n    # -------------------------\n    @staticmethod\n    def _parse_ts(x: Any) -> dt.datetime:\n        try:\n            ts = dateparser.parse(x)\n            return ts if ts and ts.tzinfo else ts.replace(tzinfo=dt.timezone.utc)\n        except Exception:\n            return dt.datetime.now(dt.timezone.utc)\n\n    @staticmethod\n    def _ensure_feedparser_installed() -> None:\n        try:\n            import feedparser  # noqa\n        except Exception:\n            import subprocess, sys\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"feedparser\", \"-q\"])\n\n    def load_news_raw(self) -> None:\n        self._ensure_feedparser_installed()\n        import feedparser  # type: ignore\n\n        items: List[Dict[str, Any]] = []\n        for src, url in self.NEWS_FEEDS.items():\n            feed = feedparser.parse(url)\n            if not feed or not getattr(feed, \"entries\", None):\n                continue\n            for e in feed.entries[:50]:\n                title = (e.get(\"title\",\"\") or \"\").strip()\n                summary = re.sub(\"<[^>]+>\", \" \", e.get(\"summary\",\"\") or \"\").strip()\n                ts = self._parse_ts(e.get(\"published\", e.get(\"updated\", \"\")))\n                items.append({\n                    \"id\": str(uuid.uuid4()),\n                    \"date\": ts,\n                    \"ticker\": None,\n                    \"title\": title[:500],\n                    \"content\": summary[:4000],\n                    \"source\": src,\n                    \"url\": e.get(\"link\",\"\"),\n                })\n\n        df_news = pd.DataFrame(items)\n        pandas_gbq.to_gbq(df_news, f\"{self.DATASET_ID}.news_raw\", project_id=self.PROJECT_ID, if_exists=\"replace\")\n        print(\"✅ news_raw loaded:\", len(df_news))\n\n    # -------------------------\n    #  Embeddings & similarity\n    # -------------------------\n    def build_news_embeddings(self) -> None:\n        assert self.EMB_MODEL, \"Call setup_models() first.\"\n        self.client.query(f\"\"\"\n        CREATE OR REPLACE TABLE `{self.PROJECT_ID}.{self.DATASET_ID}.news_embeddings` AS\n        SELECT\n          t.id,\n          t.date,\n          t.ticker,\n          t.title,\n          t.ml_generate_embedding_result AS embedding\n        FROM ML.GENERATE_EMBEDDING(\n          MODEL `{self.EMB_MODEL}`,\n          (\n            SELECT\n              id,\n              date,\n              ticker,\n              title,\n              CONCAT(IFNULL(title,''), '\\\\n', IFNULL(content,'')) AS content\n            FROM `{self.PROJECT_ID}.{self.DATASET_ID}.news_raw`\n            WHERE (title IS NOT NULL OR content IS NOT NULL)\n              AND DATE(date) >= DATE_SUB(CURRENT_DATE(), INTERVAL {self.NEWS_LOOKBACK_DAYS} DAY)\n          )\n        ) AS t;\n        \"\"\").result()\n        print(\"✅ news_embeddings rebuilt\")\n\n    def build_ticker_embeddings(self) -> None:\n        assert self.EMB_MODEL, \"Call setup_models() first.\"\n        # Ticker blobs (unchanged logic)\n        self.client.query(f\"\"\"\n        CREATE OR REPLACE TABLE `{self.PROJECT_ID}.{self.DATASET_ID}.ticker_blobs` AS\n        WITH base AS (\n          SELECT\n            ticker,\n            date,\n            PERCENTILE_CONT(z_day, 0.5) OVER (PARTITION BY ticker) AS med_z_day,\n            PERCENTILE_CONT(z_ar,  0.5) OVER (PARTITION BY ticker) AS med_z_ar,\n            ROW_NUMBER() OVER (PARTITION BY ticker ORDER BY date DESC) AS rn\n          FROM `{self.PROJECT_ID}.{self.DATASET_ID}.abnormal_returns`\n        )\n        SELECT\n          ticker,\n          CONCAT(\n            'Ticker ', ticker, ' recent context. ',\n            'Median z_day=', FORMAT('%+.2f', med_z_day),\n            ', median z_ar=', FORMAT('%+.2f', med_z_ar)\n          ) AS blob\n        FROM base\n        WHERE rn = 1;\n        \"\"\").result()\n\n        self.client.query(f\"\"\"\n        CREATE OR REPLACE TABLE `{self.PROJECT_ID}.{self.DATASET_ID}.ticker_embeddings_raw` AS\n        SELECT *\n        FROM ML.GENERATE_EMBEDDING(\n          MODEL `{self.EMB_MODEL}`,\n          (\n            SELECT\n              ticker,\n              CAST(blob AS STRING) AS content\n            FROM `{self.PROJECT_ID}.{self.DATASET_ID}.ticker_blobs`\n            WHERE blob IS NOT NULL\n          )\n        );\n        \"\"\").result()\n\n        # Detect embedding column name in ticker_embeddings_raw\n        rows = list(self.client.query(f\"\"\"\n        SELECT column_name, data_type\n        FROM `{self.PROJECT_ID}.{self.DATASET_ID}.INFORMATION_SCHEMA.COLUMNS`\n        WHERE table_name = 'ticker_embeddings_raw'\n        ORDER BY ordinal_position\n        \"\"\").result())\n        tick_emb_col = None\n        for r in rows:\n            if \"embedding\" in (r[\"column_name\"] or \"\").lower() and (r[\"data_type\"] or \"\").startswith((\"ARRAY<\",\"VECTOR<\")):\n                tick_emb_col = r[\"column_name\"]; break\n        if tick_emb_col is None:\n            raise ValueError(\"No embedding column found in ticker_embeddings_raw.\")\n\n        self.client.query(f\"\"\"\n        CREATE OR REPLACE TABLE `{self.PROJECT_ID}.{self.DATASET_ID}.ticker_embeddings` AS\n        SELECT ticker, `{tick_emb_col}` AS embedding\n        FROM `{self.PROJECT_ID}.{self.DATASET_ID}.ticker_embeddings_raw`;\n        \"\"\").result()\n        print(\"✅ ticker_embeddings ready\")\n\n    def _detect_news_embedding_col(self) -> str:\n        rows = list(self.client.query(f\"\"\"\n        SELECT column_name, data_type\n        FROM `{self.PROJECT_ID}.{self.DATASET_ID}.INFORMATION_SCHEMA.COLUMNS`\n        WHERE table_name = 'news_embeddings'\n        ORDER BY ordinal_position\n        \"\"\").result())\n        news_emb_col = None\n        for r in rows:\n            if \"embedding\" in (r[\"column_name\"] or \"\").lower() and (r[\"data_type\"] or \"\").startswith((\"ARRAY<\",\"VECTOR<\")):\n                news_emb_col = r[\"column_name\"]; break\n        if news_emb_col is None:\n            raise ValueError(\"No embedding column found in news_embeddings.\")\n        return news_emb_col\n\n    def rebuild_similarity(self) -> None:\n        news_emb_col = self._detect_news_embedding_col()\n        self.client.query(f\"\"\"\n        CREATE OR REPLACE TABLE `{self.PROJECT_ID}.{self.DATASET_ID}.news_ticker_similarity` AS\n        WITH src AS (\n          SELECT\n            n.id AS news_id,\n            t.ticker AS ticker,\n            CAST(n.{news_emb_col} AS ARRAY<FLOAT64>) AS n_emb,\n            CAST(t.embedding     AS ARRAY<FLOAT64>) AS t_emb\n          FROM `{self.PROJECT_ID}.{self.DATASET_ID}.news_embeddings` n\n          CROSS JOIN `{self.PROJECT_ID}.{self.DATASET_ID}.ticker_embeddings` t\n        ),\n        scored AS (\n          SELECT\n            news_id,\n            ticker,\n            SAFE_DIVIDE(\n              (SELECT SUM(ne * te)\n                 FROM UNNEST(n_emb) AS ne WITH OFFSET i\n                 JOIN UNNEST(t_emb) AS te WITH OFFSET j\n                   ON i = j),\n              (\n                SQRT((SELECT SUM(ne * ne) FROM UNNEST(n_emb) AS ne)) *\n                SQRT((SELECT SUM(te * te) FROM UNNEST(t_emb) AS te))\n              )\n            ) AS cosine_sim\n          FROM src\n        )\n        SELECT news_id, ticker, cosine_sim\n        FROM scored\n        WHERE cosine_sim IS NOT NULL AND cosine_sim >= {self.COSINE_MIN};\n        \"\"\").result()\n        print(\"✅ news_ticker_similarity rebuilt\")\n\n    # -------------------------\n    # Keywords (parameterized)\n    # -------------------------\n    def set_keywords(self, ticker_keywords: Dict[str, Iterable[str]]) -> None:\n        # store normalized copy\n        self.TICKER_KEYWORDS = {t.upper(): [str(k).lower() for k in ks] for t, ks in ticker_keywords.items()}\n\n    def keyword_fallback(self) -> None:\n        if not self.TICKER_KEYWORDS:\n            print(\"⚠️ No TICKER_KEYWORDS provided; skipping keyword fallback.\")\n            return\n\n        rows = [{\"ticker\": t, \"keyword\": k} for t, ks in self.TICKER_KEYWORDS.items() for k in ks]\n        to_gbq(pd.DataFrame(rows), f\"{self.DATASET_ID}.ticker_keywords\",\n               project_id=self.PROJECT_ID, if_exists=\"replace\")\n\n        self.client.query(fr\"\"\"\n        CREATE OR REPLACE TABLE `{self.PROJECT_ID}.{self.DATASET_ID}.news_ticker_keyword` AS\n        WITH n AS (\n          SELECT id, LOWER(CONCAT(' ', title, ' ', content, ' ')) AS blob\n          FROM `{self.PROJECT_ID}.{self.DATASET_ID}.news_raw`\n          WHERE date >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {self.NEWS_LOOKBACK_DAYS} DAY)\n        ),\n        k AS (SELECT ticker, LOWER(keyword) AS keyword FROM `{self.PROJECT_ID}.{self.DATASET_ID}.ticker_keywords`)\n        SELECT\n          n.id AS news_id, k.ticker, {self.COSINE_MIN} AS cosine_sim\n        FROM n JOIN k\n        ON REGEXP_CONTAINS(\n             n.blob,\n             r'(?:^|\\W)' ||\n             REGEXP_REPLACE(k.keyword, r'([\\.^$|?*+()\\[\\]{{}}])', r'\\\\\\1') ||\n             r'(?:\\W|$)'\n           );\n        \"\"\").result()\n        print(\"✅ news_ticker_keyword rebuilt\")\n\n    def link_news_events(self) -> None:\n        self.client.query(f\"\"\"\n        CREATE OR REPLACE TABLE `{self.PROJECT_ID}.{self.DATASET_ID}.news_ticker_links` AS\n        SELECT news_id, ticker, MAX(cosine_sim) AS best_sim\n        FROM (\n          SELECT * FROM `{self.PROJECT_ID}.{self.DATASET_ID}.news_ticker_similarity`\n          UNION ALL\n          SELECT * FROM `{self.PROJECT_ID}.{self.DATASET_ID}.news_ticker_keyword`\n        )\n        GROUP BY news_id, ticker;\n        \n        CREATE OR REPLACE TABLE `{self.PROJECT_ID}.{self.DATASET_ID}.event_news_links` AS\n        SELECT\n          m.ticker, m.date, m.z_day, m.z_ar,\n          nr.id AS news_id, nr.title, nr.content, nr.source, nr.url, nr.date AS news_time\n        FROM `{self.PROJECT_ID}.{self.DATASET_ID}.abnormal_returns` m\n        JOIN `{self.PROJECT_ID}.{self.DATASET_ID}.news_ticker_links` l USING (ticker)\n        JOIN `{self.PROJECT_ID}.{self.DATASET_ID}.news_raw` nr ON nr.id = l.news_id\n        WHERE (ABS(m.z_day) >= 1.96 OR ABS(m.z_ar) >= 1.96)\n          AND DATE(nr.date) BETWEEN DATE_SUB(m.date, INTERVAL {self.WINDOW_DAYS} DAY)\n                                AND DATE_ADD(m.date,  INTERVAL {self.WINDOW_DAYS} DAY);\n        \"\"\").result()\n        print(\"✅ event_news_links rebuilt\")\n\n    # -------------------------\n    # AI summaries, labels, backtest (unchanged SQL)\n    # -------------------------\n    def build_event_ai_summary(self) -> None:\n        assert self.GEN_MODEL, \"Call setup_models() first.\"\n        self.client.query(f\"\"\"\n        CREATE OR REPLACE TABLE `{self.PROJECT_ID}.{self.DATASET_ID}.event_ai_summary` AS\n        WITH grp AS (\n          SELECT\n            ticker,\n            date,\n            ANY_VALUE(z_day) AS z_day,\n            ANY_VALUE(z_ar)  AS z_ar,\n            COUNT(*) AS news_count,\n            STRING_AGG(\n              CONCAT('[', source, '] ', title, ' — ', SUBSTR(content, 1, 160)),\n              ' || '\n              ORDER BY news_time DESC\n              LIMIT 20\n            ) AS headlines_blob\n          FROM `{self.PROJECT_ID}.{self.DATASET_ID}.event_news_links`\n          GROUP BY ticker, date\n        )\n        SELECT\n          g.ticker,\n          g.date,\n          g.z_day,\n          g.z_ar,\n          g.news_count,\n          (\n            SELECT ml_generate_text_llm_result\n            FROM ML.GENERATE_TEXT(\n              MODEL `{self.GEN_MODEL}`,\n              (\n                SELECT CONCAT(\n                  'Explain in plain investor language the likely drivers behind ', g.ticker,\n                  ' on ', CAST(g.date AS STRING),\n                  '. Use light terms like beat/miss and explain briefly. ',\n                  'Keep to 2–4 sentences. Headlines: ',\n                  g.headlines_blob\n                ) AS prompt\n              ),\n              STRUCT(\n                256 AS max_output_tokens,\n                0.2 AS temperature,\n                TRUE AS flatten_json_output\n              )\n            )\n          ) AS summary\n        FROM grp AS g;\n        \"\"\").result()\n\n        self.client.query(f\"\"\"\n        CREATE OR REPLACE TABLE `{self.PROJECT_ID}.{self.DATASET_ID}.event_ai_tone` AS\n        WITH grp AS (\n          SELECT\n            ticker,\n            date,\n            STRING_AGG(\n              CONCAT('[', source, '] ', title, ' — ', SUBSTR(content, 1, 160)),\n              ' || '\n              ORDER BY news_time DESC\n              LIMIT 20\n            ) AS headlines_blob\n          FROM `{self.PROJECT_ID}.{self.DATASET_ID}.event_news_links`\n          GROUP BY ticker, date\n        )\n        SELECT\n          g.ticker,\n          g.date,\n          CASE\n            WHEN LOWER(REGEXP_REPLACE((\n              SELECT ml_generate_text_llm_result\n              FROM ML.GENERATE_TEXT(\n                MODEL `{self.GEN_MODEL}`,\n                (\n                  SELECT CONCAT(\n                    'Are the headlines net positive for equity holders? Answer True or False only. ',\n                    g.headlines_blob\n                  ) AS prompt\n                ),\n                STRUCT(\n                  16 AS max_output_tokens,\n                  0.0 AS temperature,\n                  TRUE AS flatten_json_output\n                )\n              )\n            ), r'[^a-z]', '')) = 'true'\n            THEN TRUE\n            ELSE FALSE\n          END AS is_positive\n        FROM grp AS g;\n        \"\"\").result()\n\n        print(\"✅ event_ai_summary + event_ai_tone rebuilt (CTE pre-aggregation fix)\")\n\n    def build_event_labels(self) -> None:\n        self.client.query(f\"\"\"\n        CREATE OR REPLACE TABLE `{self.PROJECT_ID}.{self.DATASET_ID}.event_label` AS\n        WITH base AS (\n          SELECT\n            a.ticker, a.date, a.z_day, a.z_ar,\n            COALESCE(s.news_count,0) AS news_count,\n            CASE\n              WHEN t.is_positive IS TRUE  THEN  1\n              WHEN t.is_positive IS FALSE THEN -1\n              ELSE 0\n            END AS ai_tilt\n          FROM `{self.PROJECT_ID}.{self.DATASET_ID}.abnormal_returns` a\n          LEFT JOIN `{self.PROJECT_ID}.{self.DATASET_ID}.event_ai_summary` s USING (ticker, date)\n          LEFT JOIN `{self.PROJECT_ID}.{self.DATASET_ID}.event_ai_tone`   t USING (ticker, date)\n          WHERE (ABS(a.z_day) >= 1.96 OR ABS(a.z_ar) >= 1.96)\n        ),\n        score AS (\n          SELECT\n            *,\n            LEAST(GREATEST(ABS(COALESCE(z_day,0)), ABS(COALESCE(z_ar,0))), 4)/4 * 0.4\n            + LEAST(news_count, 5)/5 * 0.4\n            + ((ai_tilt + 1)/2) * 0.2 AS signal_score\n          FROM base\n        )\n        SELECT\n          *,\n          CASE\n            WHEN signal_score >= 0.65 THEN 'Signal'\n            WHEN signal_score >= 0.40 THEN 'Weak'\n            ELSE 'Noise'\n          END AS label,\n          CASE\n            WHEN signal_score >= 0.65 THEN 0.75\n            WHEN signal_score >= 0.40 THEN 0.55\n            ELSE 0.35\n          END AS confidence\n        FROM score;\n        \"\"\").result()\n        print(\"✅ event_label rebuilt\")\n\n    def build_event_ai_summary_filled(self) -> None:\n        assert self.GEN_MODEL, \"Call setup_models() first.\"\n        self.client.query(f\"\"\"\n        CREATE OR REPLACE TABLE `{self.PROJECT_ID}.{self.DATASET_ID}.event_ai_summary_filled` AS\n        WITH have AS (\n          SELECT ticker, date, z_day, z_ar, news_count, summary\n          FROM `{self.PROJECT_ID}.{self.DATASET_ID}.event_ai_summary`\n        ),\n        need AS (\n          SELECT e.ticker, e.date, a.z_day, a.z_ar\n          FROM `{self.PROJECT_ID}.{self.DATASET_ID}.event_label` e\n          JOIN `{self.PROJECT_ID}.{self.DATASET_ID}.abnormal_returns` a USING(ticker, date)\n          LEFT JOIN have h USING(ticker, date)\n          WHERE h.ticker IS NULL\n        )\n        SELECT * FROM have\n        UNION ALL\n        SELECT\n          n.ticker,\n          n.date,\n          n.z_day,\n          n.z_ar,\n          0 AS news_count,\n          (\n            SELECT ml_generate_text_llm_result\n            FROM ML.GENERATE_TEXT(\n              MODEL `{self.GEN_MODEL}`,\n              (SELECT CONCAT(\n                'Explain in plain investor language a statistically significant move when no clear headlines are present. ',\n                'Use the data provided and avoid speculation. ',\n                'Say briefly whether it looks like technical flow vs. idiosyncratic news. ',\n                'z_day=', FORMAT('%+.2f', n.z_day), ', z_ar=', FORMAT('%+.2f', n.z_ar), '.'\n              ) AS prompt),\n              STRUCT(192 AS max_output_tokens, 0.2 AS temperature, TRUE AS flatten_json_output)\n            )\n          ) AS summary\n        FROM need n;\n        \"\"\").result()\n        print(\"✅ event_ai_summary_filled ready (uses stats-only when no news)\")\n\n    def build_backtest(self) -> None:\n        sql_bt = f\"\"\"\n        CREATE OR REPLACE TABLE `{self.PROJECT_ID}.{self.DATASET_ID}.forward_5d` AS\n        WITH r AS (\n          SELECT\n            ticker, date, close,\n            LEAD(close, 5) OVER (PARTITION BY ticker ORDER BY date) AS close_5d\n          FROM `{self.PROJECT_ID}.{self.DATASET_ID}.prices`\n        )\n        SELECT\n          ticker, date,\n          SAFE_DIVIDE(close_5d - close, close) AS fwd_5d\n        FROM r;\n        \n        CREATE OR REPLACE TABLE `{self.PROJECT_ID}.{self.DATASET_ID}.backtest_5d` AS\n        WITH j AS (\n          SELECT e.ticker, e.date, e.label, e.confidence, e.z_day, e.z_ar, f.fwd_5d\n          FROM `{self.PROJECT_ID}.{self.DATASET_ID}.event_label` e\n          JOIN `{self.PROJECT_ID}.{self.DATASET_ID}.forward_5d` f USING (ticker, date)\n          WHERE f.fwd_5d IS NOT NULL\n        )\n        SELECT\n          label,\n          CASE WHEN COALESCE(ABS(z_day),0) >= ABS(COALESCE(z_ar,0)) THEN SIGN(z_day) ELSE SIGN(z_ar) END AS shock_dir,\n          COUNT(*) AS n_events,\n          AVG(fwd_5d) AS avg_fwd5d,\n          AVG(CASE WHEN fwd_5d > 0 THEN 1 ELSE 0 END) AS hit_rate\n        FROM j\n        GROUP BY label, shock_dir\n        ORDER BY label, shock_dir;\n        \"\"\"\n        self.client.query(sql_bt).result()\n        print(\"✅ backtest_5d created\")\n\n    # -------------------------\n    # Readouts\n    # -------------------------\n    def verdict_panel(self, ticker: str, date: str | dt.date):\n        as_of = dt.date.fromisoformat(date) if isinstance(date, str) else date\n        q = f\"\"\"\n        WITH d AS (\n          SELECT a.ticker, a.date, a.r AS day_ret, a.z_day, a.z_ar\n          FROM `{self.PROJECT_ID}.{self.DATASET_ID}.abnormal_returns` a\n          WHERE a.ticker = @t AND a.date = @d\n        ),\n        tr AS (\n          SELECT t_stat FROM `{self.PROJECT_ID}.{self.DATASET_ID}.trend_10d`\n          WHERE ticker = @t AND date = @d\n        ),\n        lab AS (\n          SELECT label, confidence, signal_score\n          FROM `{self.PROJECT_ID}.{self.DATASET_ID}.event_label`\n          WHERE ticker = @t AND date = @d\n        ),\n        sum AS (\n          SELECT summary FROM `{self.PROJECT_ID}.{self.DATASET_ID}.event_ai_summary_filled`\n          WHERE ticker = @t AND date = @d\n        )\n        SELECT d.*, tr.t_stat, lab.label, lab.confidence, lab.signal_score, sum.summary\n        FROM d\n        LEFT JOIN tr ON TRUE\n        LEFT JOIN lab ON TRUE\n        LEFT JOIN sum ON TRUE;\n        \"\"\"\n        job = self.client.query(q, job_config=bigquery.QueryJobConfig(\n            query_parameters=[\n                bigquery.ScalarQueryParameter(\"t\",\"STRING\",ticker),\n                bigquery.ScalarQueryParameter(\"d\",\"DATE\",as_of),\n            ]\n        ))\n        df = job.result().to_dataframe()\n        if df.empty:\n            print(f\"No data for {ticker} on {as_of}\")\n        else:\n            if _HAS_DISPLAY:\n                display(df)\n            else:\n                print(df.to_string(index=False))\n\n    def recent_signals_and_backtest(self, limit: int = 200) -> None:\n        signals = self.client.query(f\"\"\"\n        SELECT s.*, e.summary\n        FROM `{self.PROJECT_ID}.{self.DATASET_ID}.event_label` s\n        LEFT JOIN `{self.PROJECT_ID}.{self.DATASET_ID}.event_ai_summary` e USING (ticker, date)\n        ORDER BY date DESC, signal_score DESC\n        LIMIT {int(limit)}\n        \"\"\").result().to_dataframe()\n\n        bt = self.client.query(f\"\"\"\n        SELECT * FROM `{self.PROJECT_ID}.{self.DATASET_ID}.backtest_5d` ORDER BY label, shock_dir\n        \"\"\").result().to_dataframe()\n\n        print(\"=== Recent Signals ===\")\n        for _, r in signals.iterrows():\n            zd, za = r.get(\"z_day\") or 0, r.get(\"z_ar\") or 0\n            direction = \"UpShock\" if (abs(zd) >= abs(za) and zd >= 0) or (abs(za) > abs(zd) and za >= 0) else \"DownShock\"\n            conf = int((r.get(\"confidence\") or 0)*100)\n            z_max = max(abs(zd), abs(za))\n            news = int(r.get(\"news_count\") or 0)\n            print(f\"{r['date']}  {r['ticker']}  {direction}  z≈{z_max:.2f}σ  |  {r['label']}  (conf {conf}%)  news={news}\")\n            if isinstance(r.get(\"summary\"), str) and r[\"summary\"].strip():\n                print(\"  ↳\", r[\"summary\"].strip()[:350])\n            print(\"-\")\n\n        print(\"\\n=== Backtest Summary (5d fwd) ===\")\n        print(bt.to_string(index=False))\n\n    # -------------------------\n    # Orchestrator\n    # -------------------------\n    def run_all(\n        self,\n        tickers: Iterable[str],\n        ticker_keywords: Optional[Dict[str, Iterable[str]]] = None,\n        benchmark: str = \"^GSPC\",\n        start: str = \"2018-01-01\",\n        end: Optional[str] = None,\n    ) -> None:\n        \"\"\"End-to-end run for any set of tickers + keywords.\"\"\"\n        # Setup models\n        self.setup_models()\n\n        # Load prices for every ticker (append-safe)\n        self.load_prices_many(tickers=tickers, benchmark=benchmark, start=start, end=end)\n\n        # Core tables & models\n        self.build_daily_returns()\n        self.build_linear_model()\n        self.build_abnormal_returns()\n        self.build_trend_10d()\n\n        # News ingest & linking\n        self.load_news_raw()\n        self.build_news_embeddings()\n        self.build_ticker_embeddings()\n        self.rebuild_similarity()\n\n        # Optional: keywords per ticker for fallback\n        if ticker_keywords:\n            self.set_keywords(ticker_keywords)\n            self.keyword_fallback()\n        else:\n            # still create empty keyword table to keep later SQL union predictable\n            print(\"ℹ️ No keywords provided; similarity-only links will be used.\")\n            self.client.query(f\"\"\"\n            CREATE OR REPLACE TABLE `{self.PROJECT_ID}.{self.DATASET_ID}.news_ticker_keyword` AS\n            SELECT CAST(NULL AS STRING) AS news_id, CAST(NULL AS STRING) AS ticker, CAST(NULL AS FLOAT64) AS cosine_sim\n            WHERE FALSE;\n            \"\"\").result()\n\n        self.link_news_events()\n\n        # AI outputs & labels\n        self.build_event_ai_summary()\n        self.build_event_labels()\n        self.build_event_ai_summary_filled()\n\n        # Backtest\n        self.build_backtest()\n\n```\n\nUse the following block to run the full pipeline with your watchlist and optional keyword fallback. `news_lookback_days` limits how far back we embed news, `cosine_min` is the similarity cutoff for linking headlines to tickers, and `window_days` controls the ±days around each event for matching news. `run_all` ingests prices/news, builds embeddings, generates summaries + tone, scores days as Signal/Weak/Noise, and creates a 5-day backtest. Then `verdict_panel` lets you inspect one date/ticker, and `recent_signals_and_backtest` prints recent labels plus the backtest table.\n\n\n```python\npipeline = MarketSignalPipeline(\n    news_lookback_days=120,   # or any value you want\n    cosine_min=0.60,          # similarity threshold\n    window_days=2             # news±event window\n)\n\ntickers = [\"AAPL\", \"MSFT\", \"NVDA\", \"GOOGL\", \"AMZN\"] \nkeywords = {\n    \"AAPL\": [\"apple\", \"iphone\", \"ipad\", \"macbook\", \"tim cook\"],\n    \"MSFT\": [\"microsoft\", \"windows\", \"azure\", \"copilot\", \"satya nadella\"],\n    \"NVDA\": [\"nvidia\", \"gpu\", \"geforce\", \"cuda\", \"h100\", \"blackwell\", \"jensen huang\"],\n    \"GOOGL\": [\"google\", \"alphabet\", \"youtube\", \"sundar pichai\"],\n    \"AMZN\":  [\"amazon\", \"aws\", \"prime\", \"jeff bezos\"],\n}\n\n# pipeline.run_all(\n#     tickers=tickers,\n#     ticker_keywords=keywords,\n#     benchmark=\"^GSPC\",\n#     start=\"2018-01-01\"\n# )\n\n# inspect a specific day/ticker\npipeline.verdict_panel(\"AAPL\", \"2025-08-08\")\npipeline.recent_signals_and_backtest(limit=50)\n\n```\n\nAfter running the pipeline on recent Signal/Weak/Noise labels by ticker and date, each with z-scores, news count, and the 2–4 sentence AI summary, plus a 5-day forward return backtest table. This view highlights top events and their likely catalysts at a glance. Note: due to free-credit limits, the screenshot reflects a static run; use your own credentials for live refresh.\n\n<center><img src=\"https://i.postimg.cc/pLXjpH40/Screenshot-2025-09-21-at-11-34-12-AM.png\"></center>\n<center><img src=\"https://i.postimg.cc/JnJD94Hh/Screenshot-2025-09-21-at-12-33-49-PM.png\"></center>\n\n\n\n# <div style=\"color:rgb(0, 103, 71);display:fill;border-radius:5px;background-color:whie;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:10px;color:rgb(0, 103, 71);overflow:hidden;margin:0;font-size:100%; \"><b id= '011'>Interactive Signal Dashboard (Demo) </b> </p></div>\n\n\nBelow is a snapshot of the dashboard; use the live link to explore signal scores by ticker, date, and news context. <i>Note:</i> due to free-credit limits, live refresh is paused—feel free to try the hosted demo, or run it with your own BigQuery credentials by following the steps mentioned above.\n\n<center><img src=\"https://i.postimg.cc/GtWLFkm3/Screenshot-2025-09-21-at-2-42-54-PM.png\"></center>\n\n\n**AAPL on 2025-08-08 (prototype run)**\n\n- **What happened (numbers):**\n- **Daily return:** **+4.24%** (day_ret = 0.042358)\n- **Raw z-score:** **1.69σ** (z_day = 1.691366) → not quite ≥ 1.96\n- **Abnormal return z-score:** **2.35σ** (z_ar = 2.351215) → **statistically significant vs. market**\n- **10-day trend t-stat:** **1.38** (t_stat = 1.378697) → mild positive trend\n- **Label:** **Noise** (signal_score ≈ **0.335**, confidence **0.35**) → below our **0.40 “Weak”** threshold\n- **Summary (model output, completed from your truncation):**\n  > **Okay, let’s stick to the data:** **AAPL** rose **~4.2%**, with a **market-adjusted abnormal return ≈ 2.35σ**, which is statistically notable. With limited headline evidence around the event window, the move looks more like **technical/flow-driven strength** than a clear idiosyncratic catalyst. The recent **10-day trend** is only mildly positive (t≈1.38), so we treat this as **noise** rather than a firm signal.\n\n**Dashboard:** [Open demo](https://noise-to-signal.streamlit.app/) , ·**GitHub:** [Dashboard Source Code](https://github.com/Kush-Trivedi/DSSMM/)\n\n\n# <div style=\"color:rgb(0, 103, 71);display:fill;border-radius:5px;background-color:whie;letter-spacing:0.1px;overflow:hidden\"><p style=\"padding:10px;color:rgb(0, 103, 71);overflow:hidden;margin:0;font-size:100%; \"><b id= '012'>Survey — BigQuery AI: Building the Future of Data </b> </p></div>\n\n\n- Team & Experience\n    - **Team Member**: Kush Trivedi\n        - **Certification**: [GCP Professional Machine Learning Engineer](https://google.accredible.com/d0c80e45-1f96-4fcc-b399-f240d7b04403?key=31082f28422ddfbbd2b46e898237f0dc0ccb65ea05bec4f18204e8819d36eefd#acc.0Z8EqVgU)\n        - Experience with **BigQuery AI**: 2 years\n        - Experience with **Google Cloud**: 5 years\n        - **Profile**: [LinkedIn](https://www.linkedin.com/in/kush-trivedi/), [Kaggle](https://www.kaggle.com/kushtrivedi14728)\n\n\n- **Feedback** on BigQuery AI (during this hackathon)\n> Overall experience was positive: the tooling is intuitive and building directly inside BigQuery reduced friction. \nThe main constraint was limited free credits, which restricted iterative debugging, broader backtesting, \nand live refresh testing. With more credits, we would have demonstrated a larger watchlist, intraday bars, and \nmore extensive parameter sweeps for threshold calibration.\n\n\n- BigQuery AI **Highlights**\n    - **In-SQL GenAI:** `ML.GENERATE_TEXT` produces 2–4 sentence investor blurbs plus a binary tone, all inside BigQuery (no ETL) with inputs/outputs stored for audit.\n    - **Embeddings & match:** `ML.GENERATE_EMBEDDING` on news + ticker context, then cosine similarity in SQL to link events ↔ headlines.\n\n","metadata":{}}]}